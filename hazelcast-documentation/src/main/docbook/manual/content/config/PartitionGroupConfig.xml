<?xml version='1.0' encoding='UTF-8'?>
<!--
  ~ Copyright (c) 2008-2013, Hazelcast, Inc. All Rights Reserved.
  ~
  ~ Licensed under the Apache License, Version 2.0 (the "License");
  ~ you may not use this file except in compliance with the License.
  ~ You may obtain a copy of the License at
  ~
  ~ http://www.apache.org/licenses/LICENSE-2.0
  ~
  ~ Unless required by applicable law or agreed to in writing, software
  ~ distributed under the License is distributed on an "AS IS" BASIS,
  ~ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  ~ See the License for the specific language governing permissions and
  ~ limitations under the License.
  -->

<sect1 xml:id="PartitionGroupConfig" version="5.0" xmlns="http://docbook.org/ns/docbook"
       xmlns:xi="http://www.w3.org/2001/XInclude"
       xmlns:xlink="http://www.w3.org/1999/xlink"
       xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
       xsi:schemaLocation="http://docbook.org/ns/docbook http://www.docbook.org/xml/5.0/xsd/docbook.xsd
       http://www.w3.org/1999/xlink http://www.w3.org/1999/xlink.xsd">
    <title>Partition Group Configuration</title>
    <para>Hazelcast distributes key objects into partitions (blocks) using a consistent hashing algorithm and those partitions are assigned to nodes. That means an entry is stored in a node which is owner of partition to that entry's key is assigned. Number of total partitions is default 271 and can be changed with configuration property <code>hazelcast.map.partition.count</code>. Along with those partitions, there are also copies of them as backups. Backup partitions can have multiple copies due to backup count defined in configuration, such as first backup partition, second backup partition etc. As a rule, a node can not hold more than one copy of a partition (ownership or backup). By default Hazelcast distributes partitions and their backup copies randomly and equally among cluster nodes assuming all nodes in the cluster are identical.
    </para>

    <para>
        <emphasis role="italic">What if some nodes share same JVM or physical machine or chassis and you want backups of these nodes to be assigned to nodes in another machine or chassis? What if processing or memory capacities of some nodes are different and you do not want equal number of partitions to be assigned to all nodes?
        </emphasis>
    </para>

    <para>
        You can group nodes in the same JVM (or physical machine) or nodes located in the same chassis. Or you can group nodes to create identical capacity. 
        We call these groups <literal>partition groups</literal>. This way partitions are assigned to those partition groups instead of single nodes. And backups of these partitions are located in another partition group.
    </para>

    <para>When you enable partition grouping, Hazelcast presents three choices to configure partition groups at the moments.
        <itemizedlist>

            <listitem>
                <para>
                First one is to group nodes automatically using IP addresses of nodes, so nodes sharing same network interface will be grouped together.
                    <programlisting language="xml"><![CDATA[
<partition-group enabled="true" group-type="HOST_AWARE" />]]></programlisting>
                    <programlisting language="java">
Config config = ...;
PartitionGroupConfig partitionGroupConfig = config.getPartitionGroupConfig();
partitionGroupConfig.setEnabled(true).setGroupType(MemberGroupType.HOST_AWARE);</programlisting>
                </para>
            </listitem>
            <listitem>
                <para>
                Second one is custom grouping using Hazelcast's interface matching configuration. This way, you can add different and multiple interfaces to a group. You can also use wildcards in interface addresses.
                    <programlisting language="xml"><![CDATA[
<partition-group enabled="true" group-type="CUSTOM">
    <member-group>
        <interface>10.10.0.*</interface>
        <interface>10.10.3.*</interface>
        <interface>10.10.5.*</interface>
    </member-group>
    <member-group>
        <interface>10.10.10.10-100</interface>
        <interface>10.10.1.*</interface>
        <interface>10.10.2.*</interface>
    </member-group
</partition-group>]]></programlisting>
                    <programlisting language="java">
Config config = ...;
PartitionGroupConfig partitionGroupConfig = config.getPartitionGroupConfig();
partitionGroupConfig.setEnabled(true).setGroupType(MemberGroupType.CUSTOM);

MemberGroupConfig memberGroupConfig = new MemberGroupConfig();
memberGroupConfig.addInterface("10.10.0.*")
    .addInterface("10.10.3.*").addInterface("10.10.5.*");

MemberGroupConfig memberGroupConfig2 = new MemberGroupConfig();
memberGroupConfig2.addInterface("10.10.10.10-100")
    .addInterface("10.10.1.*").addInterface("10.10.2.*");

partitionGroupConfig.addMemberGroupConfig(memberGroupConfig);
partitionGroupConfig.addMemberGroupConfig(memberGroupConfig2);</programlisting>
                </para>
            </listitem>
            <listitem>
                <para>
                    Third one is to give every member their own group. This gives the least amount of protection and is
                    the default configuration for a Hazelcast cluster.
                    <programlisting language="xml"><![CDATA[
<partition-group enabled="true" group-type="PER_MEMBER" />]]></programlisting>
                    <programlisting language="java">
                        Config config = ...;
                        PartitionGroupConfig partitionGroupConfig = config.getPartitionGroupConfig();
                        partitionGroupConfig.setEnabled(true).setGroupType(MemberGroupType.PER_MEMBER);</programlisting>
                </para>
            </listitem>
        </itemizedlist>
    </para>
</sect1>
