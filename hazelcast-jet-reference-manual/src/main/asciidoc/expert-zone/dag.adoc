[[dag]]
= DAG

The DAG-building API is centered around the {jet-javadoc}/core/DAG.html[`DAG`]
class. This is a pure data class and can be instantiated on its own,
without a Jet instance. This makes it simple to separate the
job-describing code from the code that manages the lifecycle of Jet
instances.

You can "compile" a Jet pipeline into a `DAG`:

[source]
----
include::{javasource}/expertzone/Dag.java[tag=s0]
----

Studying these DAGs may be a useful aid while learning the Core API.

To start building a DAG from scratch, write

[source]
----
include::{javasource}/expertzone/Dag.java[tag=s1]
----

A good practice is to structure the DAG-building code into the following
sections:

1. Create all the vertices.
2. Configure the local parallelism of vertices.
3. Create the edges.

Example:

[source]
----
include::{javasource}/expertzone/Dag.java[tag=s2]
----

<1> Create the vertices
<2> Configure local parallelism
<3> Create the edges

[[creating-a-vertex]]
== Creating a Vertex

The two mandatory elements of creating a vertex are its string
identifier and the supplier of processors. The latter can be provided in
three variants, differing in the degree of explicit control over the
lifecycle management of the processors. From simple to complex they are:

1. {jet-javadoc}/function/SupplierEx.html[`SupplierEx<Processor>`]
   directly returns processor instances from its `get()` method. It is
   expected to be stateless and return equivalent instances on each
   call. It doesn't provide any initialization or cleanup code.
2. {jet-javadoc}/core/ProcessorSupplier.html[`ProcessorSupplier`]
   returns in a single call all the processors that will run on a single
   cluster member. It may specialize each instance, for example to
   achieve local data partitioning. It is also in charge of the
   member-local lifecycle (initialization and destruction).
3. *{jet-javadoc}/core/ProcessorMetaSupplier.html[`ProcessorMetaSupplier`]*
   returns in a single call an object that will be in charge of creating
   all the processors for a vertex. Given a list of member addresses,
   the object it returns is a `Function<Address, ProcessorSupplier>`
   which will then be called with each of the addresses from the list to
   retrieve the `ProcessorSupplier` specialized for the given member.

`ProcessorMetaSupplier` is the most fundamental facility. It is a
factory of `ProcessorSupplier` s. `SupplierEx<Processor>`
exists purely as convenience over `ProcessorSupplier` for the simplest
kinds of vertices.

You make the choice which of the three to use for a particular vertex
when you implement it. When you build a DAG from already implemented
vertices, you don't have to care, or even know, which one it's using.
You'll call a factory method that returns one or the other and they will
integrate the same way into your `newVertex()` calls.

[[local-and-global-parallelism-of-vertex]]
== Local and Global Parallelism of Vertex

The vertex is implemented at runtime by one or more instances of
`Processor` on each member. Each vertex can specify how many of its
processors will run per cluster member using the `localParallelism`
property; every member will have the same number of processors. A new
`Vertex` instance has this property set to `-1`, which requests to use
the default value equal to the configured size of the cooperative thread
pool. The latter defaults to `Runtime.availableProcessors()` and is
configurable via
{jet-javadoc}/config/InstanceConfig.html#setCooperativeThreadCount-int-[`InstanceConfig.setCooperativeThreadCount()`].

In most cases the only level of local parallelism that you'll want to
explicitly configure is `1` for the cases where no parallelism is
desirable (e.g. on a source processor reading from a file).

The **total parallelism** of a vertex is the total number of its
processors running in the whole cluster. In Jet this is always equal to
_local parallelism_ times _cluster size_ because it allocates the same
number of processors on each member. This number can be critical to the
proper functioning of a vertex that represents a distributed,
partitioned data source. If you let the total parallelism of such a
vertex exceed the number of partitions in the data source, some
processors will end up with no partitions assigned to them. This has
serious consequences in an event time-based streaming job: Jet must
produce a unique value of the watermark that doesn't overtake the
timestamps coming out of any processors. A processor with no source
partitions will emit no data and its local watermark will permanently
stay at the initial value of `Long.MIN_VALUE`. This will keep the
global watermark from advancing until the _idle partition detection_
mechanism kicks in. Jet will produce no output until this happens and
the default "partition idle" timeout is 60 seconds.

[[edge-original]]
== Edge Ordinal

An edge is connected to a vertex at a given **ordinal**, which
identifies it to the vertex and its processors. When a processor
receives an item, it knows which ordinal it came from. Things are
similar on the outbound side: the processor emits an item to a given
ordinal, but also has the option to emit the same item to all ordinals.
This is the most typical case and allows easy replication of a data
stream across several edges.

When you use the
{jet-javadoc}/core/Edge.html#between-com.hazelcast.jet.core.Vertex-com.hazelcast.jet.core.Vertex-[`between(a, b)`]
edge factory, the edge will be connected at ordinal 0 at both ends. When
you need a different ordinal, use the
{jet-javadoc}/core/Edge.html#from-com.hazelcast.jet.core.Vertex-int-[`from(a, ord1).to(b, ord2)`]
form. There must be no gaps in ordinal assignment, which means a vertex
will have inbound edges with ordinals 0..N and outbound edges with
ordinals 0..M.

This example shows the usage of `between()` and `from().to()` forms to
build a DAG with one source feeding two computational vertices:

[source]
----
include::{javasource}/expertzone/Dag.java[tag=s3]
----

[[local-and-distributed-edge]]
== Local and Distributed Edge

A major choice to make in terms of routing the data coming out of a
processor is whether the candidate set of the target vertex's processors
is unconstrained, encompassing all its processors across the cluster, or
constrained to just those running on the same cluster member. You
control this with the `distributed` property of the edge. By default the
edge is local and calling the {jet-javadoc}/core/Edge.html#distributed--[`distributed()`]
method removes this restriction.

You can minimize network traffic by employing local edges. They are
implemented with the most efficient kind of concurrent queue:
single-producer, single-consumer array-backed queue. It employs
wait-free algorithms on both sides and avoids even the latency of
`volatile` writes by using `lazySet`.

A good example of employing a local-distributed edge combo is two-stage
aggregation. Here's how it looks on our <<wordcount-dag-edges, Word
Count>> example:

[source]
----
include::{javasource}/expertzone/Dag.java[tag=s4]
----

Note that only the edge from `accumulate` to `combine` is distributed.

[[routing-policies]]
== Routing Policies

The {jet-javadoc}/core/Edge.RoutingPolicy.html[routing policy]
decides which of the processors in the candidate set to route each
particular item to.

=== Unicast

This is the default routing policy, the one you get when you write

[source]
----
include::{javasource}/expertzone/Dag.java[tag=s5]
----

For each item it chooses a single destination processor with no further
restrictions on the choice. The only guarantee given by this policy is
that exactly one processor will receive the item, but Jet also takes
care to "`spray`" the items equally over all the reception candidates.

This choice makes sense when the data doesn't have to be partitioned,
usually implying a downstream vertex which can compute the result based
on each item in isolation.

=== Isolated

This is a more restricted kind of unicast policy: any given downstream
processor receives data from exactly one upstream processor. In some DAG
setups you may need to apply selective backpressure to individual
upstream processors, with this policy you can achieve it. If the
connected vertices have equal parallelism, an isolated edge creates a
one-to-one mapping between their processors, preserving the encounter
order and partitioning.

Activate this policy by calling `isolated()` on the edge:

[source]
----
include::{javasource}/expertzone/Dag.java[tag=s6]
----

=== Broadcast

A broadcasting edge sends each item to all candidate receivers. Due to
the redundancy it creates, this kind of edge isn't appropriate for the
main data stream. Its purpose is dispatching a small amount of data to
all the processors of a vertex, typically as a part of setting up before
processing the data stream. For this reason a broadcast edge is often
high-priority as well. For example, in a hash-join one vertex creates
the lookup hashtable and sends the same instance to all the processors
of the next vertex, the one that processes the main data stream.

Activate this policy by calling `broadcast()` on the edge:

[source]
----
include::{javasource}/expertzone/Dag.java[tag=s7]
----

=== Partitioned

A partitioned edge sends each item to the one processor responsible for
the item's partition ID. On a distributed edge, this processor will be
unique across the whole cluster. On a local edge, each member will have
its own processor for each partition ID.

Jet automatically assigns partitions to processors during job
initialization. The number of partitions is fixed for the lifetime of a
cluster and you configure it on the IMDG level with the system property
`hazelcast.partition.count`. The number of partitions must not be less
than the highest global parallelism of any vertex, otherwise some
processors will get no partitions.

You can also refer to the
{hz-refman}#data-partitioning[Hazelcast Reference Manual]
for more details on partitioning in Hazelcast IMDG.

This is the default algorithm to determine the partition ID of an item:

1. Apply the key extractor function defined on the edge to retrieve the
   partitioning key.
2. Serialize the partitioning key to a byte array using Hazelcast
   serialization.
3. Apply Hazelcast's standard `MurmurHash3`-based algorithm to get the
   key's hash value.
4. Partition ID is the hash value modulo the number of partitions.

The above procedure is quite CPU-intensive, but has the crucial
property of giving repeatable results across all cluster members, which
may be running on disparate JVM implementations.

Another common choice is to use Java's standard `Object.hashCode()`. It
is often significantly faster and it's safe to use on a local edge.
On a distributed edge it is not a safe strategy in general because
`hashCode()` 's contract does not require repeatable results across
JVMs or even different instances of the same JVM version. If a given
class's Javadoc explicitly specifies the hashing function it uses, then
its instances are safe to partition with `hashCode()`.

You can provide your own implementation of `Partitioner` to gain full
control over the partitioning strategy.

We use both partitioning strategies in the Word Count example:

[source]
----
include::{javasource}/expertzone/Dag.java[tag=s8]
----

Note that the local-partitioned edge uses partitioning by hash code and
the distributed edge uses the default Hazelcast partitioning. It turns
out that in this particular example we could safely use the hash code
both times, but we leave it like this for didactic purposes. Since much
less data travels towards the combiner than towards the accumulator, the
performance of the whole job is hardly affected by this choice.

=== All-To-One

The all-to-one routing policy is a special case of the `partitioned`
policy which assigns the same partition ID to all items. `allToOne(key)`
is semantically equivalent to `partitioned(t -> key)`, the former is
slightly faster because Jet avoids recalculation of the partition ID for
each stream item.

Two edges with equal key will go to the same partition and will be
processed by the same member. If you have multiple all-to-one edges in
the job or in multiple jobs, you should assign different key to each of
them so that they all don't run on the same member. On the other hand,
if you have two all-to-one edges going to the same vertex, they should
have the same key so that all the items are really processed by the same
member.

You should always set the local parallelism of the target vertex to 1,
otherwise there will be idle processors that never get any items.

On a local edge this policy doesn't make sense since simply setting the
local parallelism of the target vertex to 1 constrains the local choice
to just one processor instance.

One case where the all-to-one routing is appropriate is global
aggregation (without grouping). A single processor must see all the
data. Here's how we would set it between the first-stage and the
second-stage aggregating vertex:

[source]
----
include::{javasource}/expertzone/Dag.java[tag=s9]
----

== Priority

By default the processor receives items from all inbound edges as they
arrive. However, there are important cases where an edge must be
consumed in full to make the processor ready to accept data from other
edges. A major example is a "`hash join`" which enriches the data stream
with data from a lookup table. This can be modeled as a join of two data
streams where the _enriching_ stream contains the data for the lookup
table and must be consumed in full before consuming the stream to be
enriched.

The `priority` property controls the order of consuming the edges. Edges
are sorted by their priority number (ascending) and consumed in that
order. Edges with the same priority are consumed without particular
ordering (as the data arrives).

We can see a prioritized edge in action in the
{jet-samples}/core-api/tf-idf-core-api/src/main/java/TfIdfCoreApi.java[TF-IDF]
example:

[source]
----
include::{javasource}/expertzone/Dag.java[tag=s10]
----

The `tokenize` vertex receives a set of _stopwords_ and filters out
their occurrences from the input text. It must receive the entire set
before beginning to process the text.

=== A Fault Tolerance Caveat

As explained in the section on the <<snapshotting-callbacks, Processor>>
API, Jet takes regular snapshots of processor state when fault tolerance
is enabled. A processor will get a special item in its input stream,
called a _barrier_. When working in the _exactly once_ mode, as soon as
it receives it, it must stop pulling the data from that stream and
continue pulling from all other streams until it receives the same
barrier in all of them, and then emit its state to the snapshot storage.
This is in direct contradiction with the contract of edge
prioritization: the processor is not allowed to consume any other
streams before having fully exhausted the prioritized ones.

For this reason Jet does not initiate a snapshot until all the
higher-priority edges have been fully consumed.

Although strictly speaking this only applies to the _exactly once_ mode,
Jet postpones taking the snapshot in _at least once_ mode as well. Even
though the snapshot could begin early, it would still not be able to
complete until the processor has consumed all the prioritized edges,
started consuming non-prioritized ones, and received the barrier in all
of them. The result would be many more items processed twice after the
restart.

[[fine-tuning-edges]]
== Fine-Tuning Edges

Edges can be configured with an
{jet-javadoc}/config/EdgeConfig.html[`EdgeConfig`]
instance, which specifies additional fine-tuning parameters. For
example,

[source]
----
include::{javasource}/expertzone/Dag.java[tag=s11]
----

Please refer to the Javadoc of
{jet-javadoc}/config/EdgeConfig.html[`EdgeConfig`]
for details.
